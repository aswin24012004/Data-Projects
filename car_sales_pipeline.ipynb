# 📦 Import necessary libraries
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, trim
from sqlalchemy import create_engine
import pandas as pd
import os

# 📂 Input folder containing multiple CSV files
input_file_path = "/content/sample_data/data/*.csv"
sqlite_db_path = "car_sales.db"
csv_output_path = "car_sales_data.csv"

# 🧩 Step 1: Extract
def extract_data(file_path):
    spark = SparkSession.builder \
        .appName("CarSales_ETL") \
        .config("spark.driver.extraJavaOptions", "--add-opens=java.base/java.lang=ALL-UNNAMED") \
        .getOrCreate()

    spark.sparkContext.setLogLevel("ERROR")
    print("✅ Spark session created successfully!")

    # Read multiple CSVs
    df = spark.read.option("header", True).csv(file_path)
    print("📥 Data extracted successfully.")
    return df

# 🧹 Step 2: Transform
def transform_data(df):
    print("🔄 Transforming data...")

    # Drop rows with missing critical values
    df = df.dropna(subset=["Model", "Region", "Price_USD", "Sales_Volume"])

    # Clean string columns
    df = df.withColumn("Model", trim(col("Model")))
    df = df.withColumn("Region", trim(col("Region")))

    # Convert numeric columns
    df = df.withColumn("Price_USD", col("Price_USD").cast("double"))
    df = df.withColumn("Sales_Volume", col("Sales_Volume").cast("double"))

    # Calculate Total Revenue
    df = df.withColumn("Total_Revenue", col("Price_USD") * col("Sales_Volume"))

    print("✅ Data transformed successfully!")
    return df

# 💾 Step 3: Load to SQLite and export CSV
def load_to_sqlite(df):
    print("📤 Loading data into SQLite database...")

    # Delete old DB file if it exists
    if os.path.exists(sqlite_db_path):
        os.remove(sqlite_db_path)
        print("🧹 Old SQLite database deleted.")

    # Convert Spark DataFrame to Pandas
    pdf = df.toPandas()

    # Create SQLite engine (database file stored in Colab environment)
    engine = create_engine(f"sqlite:///{sqlite_db_path}")

    # Write data to SQLite
    pdf.to_sql("car_sales_data", con=engine, if_exists="replace", index=False)
    print("✅ Data successfully loaded into SQLite database (car_sales.db).")

    # Export to CSV for download
    pdf.to_csv(csv_output_path, index=False)
    print("✅ Data exported to CSV (car_sales_data.csv).")

    # For Colab download
    try:
        from google.colab import files
        files.download(csv_output_path)
    except:
        print("⚠️ Running outside Colab — download manually.")

    # Optional: Verify saved data
    verify_data(engine)

# 🔍 Optional verification function
def verify_data(engine):
    print("\n🧾 Verifying saved data from SQLite:")
    query = "SELECT * FROM car_sales_data LIMIT 5;"
    result = pd.read_sql(query, con=engine)
    print(result)

# 🚀 ETL Pipeline Execution
if __name__ == "__main__":
    # Step 1: Extract
    df = extract_data(input_file_path)
    print(f"📊 Total records before cleaning: {df.count()}")

    # Step 2: Transform
    df_transformed = transform_data(df)
    print(f"📊 Total records after cleaning: {df_transformed.count()}")

    # Step 3: Load
    load_to_sqlite(df_transformed)

    print("\n🎯 ETL Pipeline executed successfully!")
